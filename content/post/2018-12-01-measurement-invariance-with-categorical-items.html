---
title: Measurement Invariance with Categorical Items
author: Giacomo Mason
date: '2018-12-09'
slug: measurement-invariance-with-categorical-items
categories:
  - R
  - statistics
tags: [R,factor analysis,measurement invariance]
image:
  caption: ''
  focal_point: ''
bibliography: cohorts.bib

---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<p><em>This post is inspired by my recent work “Inequality in socioemotional skills: a cross-cohort comparison” published as a working paper <a href="http://127.0.0.1:4709/publication/cohorts/cohorts/">here</a>.</em></p>
<div id="what-is-measurement-invariance" class="section level4">
<h4>What is Measurement Invariance?</h4>
<p>Suppose you’re an HR executive in a large firm, and you want to know: Are workers in your New York office more or less happy with their job than your employees in Mumbai? So you ask both offices to fill in a questionnaire on job satisfaction. The questionnaire is a short version of the <a href="http://shell.cas.usf.edu/~pspector/scales/jsspag.html">Job Satisfaction Survey</a>, aiming to capture two dimensions: satisfaction with the nature of work, and satisfaction with operating conditions. There is a total of 8 items:</p>
<table>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;color: white;background-color: #576172;">
</td>
<td style="text-align:left;font-weight: bold;color: white;background-color: #576172;">
Nature of work
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
I sometimes feel my job is meaningless.
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="2" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
I like doing the things I do at work.
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="3" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
I feel a sense of pride in doing my job.
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="4" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
My job is enjoyable.
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;color: white;background-color: #576172;">
</td>
<td style="text-align:left;font-weight: bold;color: white;background-color: #576172;">
Operating Conditions
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="5" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
Many of our rules and procedures make doing a good job difficult.
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="6" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
My efforts to do a good job are seldom blocked by red tape.
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="7" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
I have too much to do at work.
</td>
</tr>
<tr>
<td style="text-align:left;">
<ol start="8" style="list-style-type: decimal">
<li></td>
<td style="text-align:left;">
I have too much paperwork.
</td>
</tr>
</tbody>
</table></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<p>Each employee can endorse each statement on three levels: <em>Do not agree</em> (0), <em>Neither agree nor disagree</em> (1), <em>Agree</em> (2).</p>
<p>You might be tempted to take the answers, compute a score for each employee, and compare the mean score in NYC with the mean score in Mumbai. Now consider the following statements that are part of your questionnaire:</p>
<ul>
<li><em>My efforts to do a good job are seldom blocked by red tape</em></li>
<li><em>I sometimes feel my job is meaningless</em></li>
</ul>
<p>There are many reasons to suspect that the interpretation of these statements might be very different between the US and India, due to different cultural norms, reference points, and expectations. In effect, two employees with the same answers to these questions, one in NYC and one in Mumbai, might have very different levels of job satisfaction!</p>
<p>Is job satisfaction being measured by your questionnaire in the same way in your two offices? This is a matter of <strong>measurement invariance</strong>. If your questionnaire is not invariant across these two groups, your interpretation of the scores might be meaningless, or even misleading.</p>
</div>
<div id="testing-mi" class="section level4">
<h4>Testing MI</h4>
<p>Luckily, measurement invariance is a <em>testable</em> statistical property. In this post, I focus on testing MI using a factor analysis approach.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> The basic idea is to estimate a series of nested factor models that place increasing restrictions on parameters across groups – e.g. the NYC and Mumbai offices in the previous example.</p>
<p>A bit more formally, say that <span class="math inline">\(X^*_{ijg}\)</span> denotes the answer to item <span class="math inline">\(i\)</span> of your questionnaire by employee <span class="math inline">\(j\)</span> in group <span class="math inline">\(g\)</span>. Factor analysis models the response to each item as a function of a person-specific latent factor <span class="math inline">\(\theta_{jg}\)</span> (in this case, job satisfaction), some item-specific parameters – intercepts <span class="math inline">\(\nu_{ig}\)</span> and factor loadings <span class="math inline">\(\lambda_{ig}\)</span> – and a random error <span class="math inline">\(u_{ijg}\)</span>.</p>
<p><span class="math display">\[X^*_{ijg} =\nu_{ig} + \lambda_{ig}\theta_{jg} + u_{ijg}\]</span></p>
<p>Everything is relatively easy-peasy when you observe continuous ratings <span class="math inline">\(X^*\)</span> directly, such as for example how much the employee agrees with each statement on a scale from 1 to 100. In this case, you estimate:</p>
<ol style="list-style-type: decimal">
<li>A <em>configural invariance</em> model – which places the minimal restrictions for identification – where both intercepts <span class="math inline">\(\nu\)</span> and loadings <span class="math inline">\(\lambda\)</span> are left free to vary across groups <span class="math inline">\(g\)</span></li>
<li>A <em>metric invariance</em> model, which constrains the loadings <span class="math inline">\(\lambda\)</span> to be equal across groups</li>
<li>A <em>scalar invariance</em> model, which additionally constrains intercepts <span class="math inline">\(\nu\)</span> to be equal across groups</li>
<li>A <em>full/uniqueness invariance</em> model, which additionally constrains the variances of the error terms <span class="math inline">\(u\)</span></li>
</ol>
<p>You can then compare the fit of these models and check which level of invariance is satisfied. This can be done by comparing absolute or relative fit indices – like the RMSEA, Tucker-Lewis Index, Comparative Fit Index, among others. If the scalar invariance model fits as well as the configural invariance model, you can safely compare the means of the latent variables. This is a very brief treatment of the issue: if you want to know more, good places to start are <span class="citation">Vandenberg and Lance (2000)</span> or <span class="citation">Sass (2011)</span>.</p>
</div>
<div id="measurement-invariance-with-categorical-measures" class="section level4">
<h4>Measurement Invariance with categorical measures</h4>
<p>Things get a bit less easy-peasy when you only observe discrete/categorical manifestations of the questionnaire items. For example, each item might be recorded on a 5-point Likert-type scale (from “completely disagree” to “completely agree”), or just in a binary fashion (yes/no). Treating these as continuous is not a good idea, as it implies a potentially dangerous linear approximation.</p>
<p>In our HR example, employees filling your job satisfaction questionnaire could endorse each statement on three levels: <em>Do not agree</em> (0), <em>Neither agree nor disagree</em> (1), <em>Agree</em> (2). All you observe are these ordered values <span class="math inline">\(X_{ijg}\)</span>, instead of the continuous <span class="math inline">\(X^*_{ijg}\)</span>. You thus need to introduce additional item-specific threshold parameters <span class="math inline">\(\tau_{s,ig}\)</span> to map the observed <span class="math inline">\(X\)</span> to the unobserved <span class="math inline">\(X^*\)</span>, as follows:</p>
<p><span class="math display">\[X_{ijg} = s \qquad \text{if} \; \tau_{s,ig} \leq X^*_{ijg} &lt; \tau_{s+1,ig} \qquad \text{for} \; s=0,1,2.\]</span></p>
<p>For the statisticians and econometricians in the room, this formulation is very much analogous to a discrete choice model, like for example ordered probit or logit. This additional set of parameters complicates identification, because – just like in an ordered probit model – it requires additional normalisations. Thankfully, a recent <em>Psychometrika</em> paper by <span class="citation">Wu and Estabrook (2016)</span> lays out the conditions for identification in this type of model in great detail. The R syntax below follows their approach closely.</p>
</div>
<div id="an-example-in-r" class="section level3">
<h3>An example in R</h3>
<p>Ok now, how do we conduct this measurement invariance analysis in R? Here I will be using the brilliant <code>lavaan</code> package, in particular its <code>cfa</code> function which estimates Confirmatory Factor Analysis models.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>library(lavaan)</code></pre>
<p>I’ve simulated some data for a thousand employees in our firm, 400 in NYC and 400 in Mumbai. For each of the employees, there’s a response to eight items (<span class="math inline">\(X_{1jg}\)</span> to <span class="math inline">\(X_{8jg}\)</span>) in three categories:</p>
<pre class="r"><code>summary(sample)</code></pre>
<pre><code>##         x1             x2             x3             x4     
##  Disagree:303   Disagree:264   Disagree:375   Disagree:269  
##  Neither :159   Neither :188   Neither :148   Neither :104  
##  Agree   :338   Agree   :348   Agree   :277   Agree   :427  
##         x5             x6             x7             x8         office   
##  Disagree:457   Disagree:180   Disagree:301   Disagree:398   NYC   :400  
##  Neither :119   Neither :139   Neither :263   Neither :179   Mumbai:400  
##  Agree   :224   Agree   :481   Agree   :236   Agree   :223</code></pre>
<div id="the-configural-model" class="section level4">
<h4>The configural model</h4>
<p>The first model we estimate is always the configural model. Again, this is a model where we only assume that the factor structure is common among groups, i.e. in both groups the same items measure the same latent constructs. In our case, the first four items (<span class="math inline">\(X_{1jg}\)</span> to <span class="math inline">\(X_{4jg}\)</span>) measure the “Nature of work” construct, while the last four measure “Operating Conditions”.</p>
<p>There are three equivalent parameterisations that we can employ to identify the parameters of the configural model. For more details, see Wu and Estabrook (2016) and Appendix B of our <a href="http://127.0.0.1:4709/publication/cohorts/cohorts/">working paper</a>, but here’s the gist of it:</p>
<ul>
<li>The <strong>Theta</strong> parameterisation, where the variances of the unique errors are restricted to 1.</li>
<li>The <strong>Delta</strong> parameterisation, where it’s the implied variance of the latent measures <span class="math inline">\(X^*\)</span> that is fixed to unity.</li>
<li>An “<strong>anchored</strong>” parameterisation is also available, as suggested by <span class="citation">Millsap and Yun-Tein (2004)</span>, where one reference indicator is selected for each latent factor. We will not consider it here.</li>
</ul>
<p>Let’s start from writing down the <code>lavaan</code> syntax for the Theta parameterisation. We will consider the first group (the NYC office) as the <em>reference</em> group, and the second group (Mumbai) as the <em>target</em> group. This is an arbitrary choice and does not affect our conclusions.</p>
<pre class="r"><code>configural_theta &lt;- &quot;
F1 =~ x1 + x2 + x3 + x4
F2 =~ x5 + x6 + x7 + x8
x1 | t1 + t2
x2 | t1 + t2
x3 | t1 + t2
x4 | t1 + t2
x5 | t1 + t2
x6 | t1 + t2
x7 | t1 + t2
x8 | t1 + t2
x1 ~~ c(1, 1)*x1
x2 ~~ c(1, 1)*x2
x3 ~~ c(1, 1)*x3
x4 ~~ c(1, 1)*x4
x5 ~~ c(1, 1)*x5
x6 ~~ c(1, 1)*x6
x7 ~~ c(1, 1)*x7
x8 ~~ c(1, 1)*x8
x1 ~ c(0, 0)*1
x2 ~ c(0, 0)*1
x3 ~ c(0, 0)*1
x4 ~ c(0, 0)*1
x5 ~ c(0, 0)*1
x6 ~ c(0, 0)*1
x7 ~ c(0, 0)*1
x8 ~ c(0, 0)*1
F1 ~~ c(1,1)*F1
F2 ~~ c(1,1)*F2
F1 ~ c(0, 0)*1
F2 ~ c(0, 0)*1
F1 ~~ NA*F2
&quot;</code></pre>
<p>As is practice when doing multigroup analysis, we can fix values for parameters by specifying a vector <code>c()</code> with as many elements as the number of groups – in our case, two. Some things to note:</p>
<ul>
<li>With <code>F1 =~ x1 + x2 + x3 + x4</code> I specify that the first 4 items load on the first factor, that I call <code>F1</code>. But <strong>watch out</strong>! Without specifying, <code>lavaan</code> will automatically fix the first loading to unity, which is not what either the Theta or Delta parameterisations require. There are a few ways to tell the software to actually estimate these parameters:
<ul>
<li>prefacing <code>x1</code> and <code>x5</code> in the model syntax by <code>c(NA,NA)</code>;</li>
<li>switching off the automatic fixing of the first loading, by specifying <code>auto.fix.first = FALSE</code> in the call to the <code>lavaan</code> function – which is what we will do in this post;</li>
<li>specifying <code>std.lv = TRUE</code> in the call, which standardises the mean and variance of the latent factors instead of the first loadings – which we will avoid, since we do want to specify different means and variances down the line;</li>
</ul></li>
<li>The syntax <code>x1 | t1 + t2</code> defines the threshold parameters. There are two thresholds per item, as each item has three levels. It is probably superflous since <code>lavaan</code> understands this itself from the levels of the factors in the data, but I specify this for clarity.</li>
<li>The syntax <code>x1 ~~ c(1, 1)*x1</code> sets the variances of the unique error terms to unity, as required by the Theta parameterisation.</li>
<li><code>x1 ~ c(0, 0)*1</code> sets the intercepts to zero in both groups for each item. This is also superflous.</li>
<li>Finally, <code>F1 ~~ c(1,1)*F1</code> sets the variance of the first factor to 1, while <code>F1 ~ c(0, 0)*1</code> sets its mean to zero, in both groups. The same is done with the second factor. <code>F1 ~~ NA*F2</code> makes sure that the covariance between the factors gets estimated.</li>
</ul>
<p>As you see, once you understand how <code>lavaan</code> syntax works, it’s easy to get full control of how you specify your model. The Delta parameterisation is very similar, but we use <code>x1 ~*~ c(1, 1)*x1</code> to restrict the latent measures’ implied variances instead of the variances of the uniqueness.</p>
<pre class="r"><code>configural_delta &lt;- &quot;
F1 =~ x1 + x2 + x3 + x4
F2 =~ x5 + x6 + x7 + x8
x1 | t1 + t2
x2 | t1 + t2
x3 | t1 + t2
x4 | t1 + t2
x5 | t1 + t2
x6 | t1 + t2
x7 | t1 + t2
x8 | t1 + t2
x1 ~*~ c(1, 1)*x1
x2 ~*~ c(1, 1)*x2
x3 ~*~ c(1, 1)*x3
x4 ~*~ c(1, 1)*x4
x5 ~*~ c(1, 1)*x5
x6 ~*~ c(1, 1)*x6
x7 ~*~ c(1, 1)*x7
x8 ~*~ c(1, 1)*x8
x1 ~ c(0, 0)*1
x2 ~ c(0, 0)*1
x3 ~ c(0, 0)*1
x4 ~ c(0, 0)*1
x5 ~ c(0, 0)*1
x6 ~ c(0, 0)*1
x7 ~ c(0, 0)*1
x8 ~ c(0, 0)*1
F1 ~~ c(1,1)*F1
F2 ~~ c(1,1)*F2
F1 ~ c(0, 0)*1
F2 ~ c(0, 0)*1
F1 ~~ NA*F2
&quot;</code></pre>
<p>These two parameterisations are statistically equivalent. They are just three of the potentially infinite ways in which we can identify the model! This is because of the <em>factor indeterminacy</em> common to all latent variable models.</p>
<p>How can we show that they are indeed equivalent? Let’s estimate the models and compare their fit by their <span class="math inline">\(\chi^2\)</span> value. We could use the confirmatory factor analysis (<code>cfa()</code>) function – a user-friendly wrapper – but it does not take the <code>auto.fix.first</code> argument we need for the loadings. So let’s use the more low-level <code>lavaan()</code> function, which takes the same syntax as first argument:</p>
<pre class="r"><code>m_conf_theta &lt;- lavaan(configural_theta, 
                    data = sample, group = &quot;office&quot;, 
                    parameterization=&quot;theta&quot;, estimator=&quot;wlsmv&quot;,
                    auto.fix.first = FALSE)
m_conf_delta &lt;- lavaan(configural_delta, 
                    data = sample, group = &quot;office&quot;, 
                    parameterization=&quot;delta&quot;, estimator=&quot;wlsmv&quot;,
                    auto.fix.first = FALSE)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Num. parameters
</th>
<th style="text-align:right;">
Chi-squared
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Theta
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
38.93141
</td>
</tr>
<tr>
<td style="text-align:left;">
Delta
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
38.93141
</td>
</tr>
</tbody>
</table>
</div>
<div id="estimating-nested-models" class="section level4">
<h4>Estimating nested models</h4>
<p>After the configural model, we can proceed with estimating the nested models. We will then test these against the configural model and assess their relative fit. For this post, we will stick with the Theta parameterisation, but the same approach can be done using Delta. Wu and Estabrook (2016) suggest estimating the following sequence of models:</p>
<ol style="list-style-type: decimal">
<li><em>Threshold Invariance</em> – Note that this is statistically equivalent to the configural model when data is ternary (three categories) like in our case. You can see that the syntax to restrict parameters to be equal requires giving them the same name (e.g. <code>t1_1</code> for the first threshold on the first item in both groups).</li>
</ol>
<pre class="r"><code>t_inv_theta &lt;- &quot;
F1 =~ x1 + x2 + x3 + x4
F2 =~ x5 + x6 + x7 + x8
x1 | c(t1_1, t1_1)*t1 + c(t2_1, t2_1)*t2
x2 | c(t1_2, t1_2)*t1 + c(t2_2, t2_2)*t2
x3 | c(t1_3, t1_3)*t1 + c(t2_3, t2_3)*t2
x4 | c(t1_4, t1_4)*t1 + c(t2_4, t2_4)*t2
x5 | c(t1_5, t1_5)*t1 + c(t2_5, t2_5)*t2
x6 | c(t1_6, t1_6)*t1 + c(t2_6, t2_6)*t2
x7 | c(t1_7, t1_7)*t1 + c(t2_7, t2_7)*t2
x8 | c(t1_8, t1_8)*t1 + c(t2_8, t2_8)*t2
x1 ~~ c(1, NA)*x1
x2 ~~ c(1, NA)*x2
x3 ~~ c(1, NA)*x3
x4 ~~ c(1, NA)*x4
x5 ~~ c(1, NA)*x5
x6 ~~ c(1, NA)*x6
x7 ~~ c(1, NA)*x7
x8 ~~ c(1, NA)*x8
x1  ~ c(0, NA)*1
x2  ~ c(0, NA)*1
x3  ~ c(0, NA)*1
x4  ~ c(0, NA)*1
x5  ~ c(0, NA)*1
x6  ~ c(0, NA)*1
x7  ~ c(0, NA)*1
x8  ~ c(0, NA)*1
F1 ~~ c(1,1)*F1
F2 ~~ c(1,1)*F2
F1 ~ c(0, 0)*1
F2 ~ c(0, 0)*1
F1 ~~ NA*F2
&quot;
m_t_inv_theta &lt;- lavaan(t_inv_theta, 
                        data = sample, group = &quot;office&quot;, 
                        parameterization=&quot;theta&quot;, estimator=&quot;wlsmv&quot;,
                        auto.fix.first = FALSE)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><em>Threshold and loading invariance</em>: This level of invariance restricts both thresholds and loadings to be the same across groups, and frees the variance of the latent variable the target group(s).</li>
</ol>
<pre class="r"><code>tl_inv_theta &lt;- &quot;
F1 =~ c(l1, l1)*x1 + c(l2,l2)*x2 + c(l3,l3)*x3 + c(l4,l4)*x4
F2 =~ c(l2, l2)*x5 + c(l6,l6)*x6 + c(l7,l7)*x7 + c(l8,l8)*x8
x1 | c(t1_1, t1_1)*t1 + c(t2_1, t2_1)*t2
x2 | c(t1_2, t1_2)*t1 + c(t2_2, t2_2)*t2
x3 | c(t1_3, t1_3)*t1 + c(t2_3, t2_3)*t2
x4 | c(t1_4, t1_4)*t1 + c(t2_4, t2_4)*t2
x5 | c(t1_5, t1_5)*t1 + c(t2_5, t2_5)*t2
x6 | c(t1_6, t1_6)*t1 + c(t2_6, t2_6)*t2
x7 | c(t1_7, t1_7)*t1 + c(t2_7, t2_7)*t2
x8 | c(t1_8, t1_8)*t1 + c(t2_8, t2_8)*t2
x1 ~~ c(1, NA)*x1
x2 ~~ c(1, NA)*x2
x3 ~~ c(1, NA)*x3
x4 ~~ c(1, NA)*x4
x5 ~~ c(1, NA)*x5
x6 ~~ c(1, NA)*x6
x7 ~~ c(1, NA)*x7
x8 ~~ c(1, NA)*x8
x1  ~ c(0, NA)*1
x2  ~ c(0, NA)*1
x3  ~ c(0, NA)*1
x4  ~ c(0, NA)*1
x5  ~ c(0, NA)*1
x6  ~ c(0, NA)*1
x7  ~ c(0, NA)*1
x8  ~ c(0, NA)*1
F1 ~~ c(1,NA)*F1
F2 ~~ c(1,NA)*F2
F1 ~ c(0, 0)*1
F2 ~ c(0, 0)*1
F1 ~~ NA*F2
&quot;
m_tl_inv_theta &lt;- lavaan(tl_inv_theta, 
                         data = sample, group = &quot;office&quot;, 
                         parameterization=&quot;theta&quot;, estimator=&quot;wlsmv&quot;,
                         auto.fix.first = FALSE)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><em>Threshold, loading, and intercept invariance</em> – which further restricts the intercepts, and frees the mean as well as the intercept of the latent variables in the target group(s).</li>
</ol>
<pre class="r"><code>tli_inv_theta &lt;- &quot;
F1 =~ c(l1, l1)*x1 + c(l2,l2)*x2 + c(l3,l3)*x3 + c(l4,l4)*x4
F2 =~ c(l2, l2)*x5 + c(l6,l6)*x6 + c(l7,l7)*x7 + c(l8,l8)*x8
x1 | c(t1_1, t1_1)*t1 + c(t2_1, t2_1)*t2
x2 | c(t1_2, t1_2)*t1 + c(t2_2, t2_2)*t2
x3 | c(t1_3, t1_3)*t1 + c(t2_3, t2_3)*t2
x4 | c(t1_4, t1_4)*t1 + c(t2_4, t2_4)*t2
x5 | c(t1_5, t1_5)*t1 + c(t2_5, t2_5)*t2
x6 | c(t1_6, t1_6)*t1 + c(t2_6, t2_6)*t2
x7 | c(t1_7, t1_7)*t1 + c(t2_7, t2_7)*t2
x8 | c(t1_8, t1_8)*t1 + c(t2_8, t2_8)*t2
x1 ~~ c(1, NA)*x1
x2 ~~ c(1, NA)*x2
x3 ~~ c(1, NA)*x3
x4 ~~ c(1, NA)*x4
x5 ~~ c(1, NA)*x5
x6 ~~ c(1, NA)*x6
x7 ~~ c(1, NA)*x7
x8 ~~ c(1, NA)*x8
x1  ~ c(0, 0)*1
x2  ~ c(0, 0)*1
x3  ~ c(0, 0)*1
x4  ~ c(0, 0)*1
x5  ~ c(0, 0)*1
x6  ~ c(0, 0)*1
x7  ~ c(0, 0)*1
x8  ~ c(0, 0)*1
F1 ~~ c(1,NA)*F1
F2 ~~ c(1,NA)*F2
F1 ~ c(0, NA)*1
F2 ~ c(0, NA)*1
F1 ~~ NA*F2
&quot;
m_tli_inv_theta &lt;- lavaan(tli_inv_theta, 
                          data = sample, group = &quot;office&quot;, 
                          parameterization=&quot;theta&quot;, estimator=&quot;wlsmv&quot;,
                          auto.fix.first = FALSE)</code></pre>
</div>
<div id="comparing-fit" class="section level4">
<h4>Comparing fit</h4>
<p>Now that we have estimated these increasingly restricted models, how do we assess how well they fit? Remember that our reference (baseline) model is always the configural model, to which all subsequent models should be compared.</p>
<p>A popular method is to conduct a chi-squared test of each model against the configural. This can easily be done with the <code>anova()</code> method:</p>
<pre class="r"><code>anova(m_conf_theta, m_tl_inv_theta, m_tli_inv_theta)</code></pre>
<pre><code>## Scaled Chi Square Difference Test (method = &quot;satorra.2000&quot;)
## 
##                 Df AIC BIC   Chisq Chisq diff Df diff Pr(&gt;Chisq)    
## m_conf_theta    38          38.931                                  
## m_tl_inv_theta  45          55.736     17.853       7    0.01265 *  
## m_tli_inv_theta 51         128.445     59.456       6  5.804e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see that the hypothesis that the model with restricted loadings and thresholds fits as well as the configural model has a quite low p value, indicating possible lack of <em>metric</em> invariance. Our model with restricted intercepts fails the invariance test even more severely.</p>
<p>When we have relatively large samples, the <span class="math inline">\(\chi^2\)</span> test is known to over-reject invariance. It is thus usually accompanied by the analysis of alternative fit indices (AFIs). There is a lot of debate about the appropriate cutoffs to use (see <span class="citation">Cheung and Rensvold (2002)</span> for example), especially in the categorical measures case. My suggestion is to interpret them “holistically” rather than adhering to any strict guideline.</p>
<p>Here’s a table with some commonly used indices for each of our fitted models – including the root mean squared error of approximation (RMSEA), the comparative fit index (CFI), and the Tucker-Lewis index (TLI).</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Params
</th>
<th style="text-align:right;">
<span class="math inline">\(\chi^2\)</span>
</th>
<th style="text-align:right;">
RMSEA
</th>
<th style="text-align:right;">
CFI
</th>
<th style="text-align:right;">
TLI
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Configural
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
38.931
</td>
<td style="text-align:right;">
0.008
</td>
<td style="text-align:right;">
0.999
</td>
<td style="text-align:right;">
0.998
</td>
</tr>
<tr>
<td style="text-align:left;">
Threshold Invariance
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
38.931
</td>
<td style="text-align:right;">
0.008
</td>
<td style="text-align:right;">
0.999
</td>
<td style="text-align:right;">
0.998
</td>
</tr>
<tr>
<td style="text-align:left;">
Threshold and loading invariance
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
55.736
</td>
<td style="text-align:right;">
0.024
</td>
<td style="text-align:right;">
0.985
</td>
<td style="text-align:right;">
0.981
</td>
</tr>
<tr>
<td style="text-align:left;">
Threshold, loading, and intercept invariance
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
128.445
</td>
<td style="text-align:right;">
0.062
</td>
<td style="text-align:right;">
0.890
</td>
<td style="text-align:right;">
0.880
</td>
</tr>
</tbody>
</table>
<p>We can see that:</p>
<ul>
<li>Our configural model fits the data very well.</li>
<li>As shown in <span class="citation">Wu and Estabrook (2016)</span>, restricting only the thresholds gives a statistically equivalent model with the same fit.</li>
<li>Restriction of both threshold and loadings delivers a more restrictive model, that seems to fit comparably to the configural one.</li>
<li>Further restricting the loadings provide a much worse fit.</li>
</ul>
<p>In our business example, we would conclude that the same questionnaire administered in our US and India offices is not really capturing in the same way how our employees feel about “Nature of Work” and “Operating Conditions”.</p>
<p>Obviously, this is simulated data, and the real world outside our laptops will be way messier!</p>
<p>The complete code for the post is in the .Rmd file on <a href="https://github.com/masongcm/gmwebsite/blob/master/content/post/2018-12-01-measurement-invariance-with-categorical-items.Rmd">my GitHub</a>.</p>
</div>
<div id="references" class="section level4 unnumbered">
<h4>References</h4>
<div id="refs" class="references">
<div id="ref-Cheung2002">
<p>Cheung, Gordon W., and Roger B. Rensvold. 2002. “Evaluating Goodness-of-Fit Indexes for Testing Measurement Invariance.” <em>Structural Equation Modeling: A Multidisciplinary Journal</em> 9 (2): 233–55. doi:<a href="https://doi.org/10.1207/S15328007SEM0902_5">10.1207/S15328007SEM0902_5</a>.</p>
</div>
<div id="ref-Millsap2004">
<p>Millsap, Roger E., and Jenn Yun-Tein. 2004. “Assessing Factorial Invariance in Ordered-Categorical Measures.” <em>Multivariate Behavioral Research</em> 39 (3): 479–515. doi:<a href="https://doi.org/10.1207/S15327906MBR3903_4">10.1207/S15327906MBR3903_4</a>.</p>
</div>
<div id="ref-Sass2011">
<p>Sass, Daniel A. 2011. “Testing Measurement Invariance and Comparing Latent Factor Means Within a Confirmatory Factor Analysis Framework.” <em>Journal of Psychoeducational Assessment</em> 29 (4): 347–63.</p>
</div>
<div id="ref-Vandenberg2000a">
<p>Vandenberg, Robert J., and Charles E. Lance. 2000. “A Review and Synthesis of the Measurement Invariance Literature: Suggestions, Practices, and Recommendations for Organizational Research.” <em>Organizational Research Methods</em> 3 (1): 4–70. doi:<a href="https://doi.org/10.1177/109442810031002">10.1177/109442810031002</a>.</p>
</div>
<div id="ref-Wu2016a">
<p>Wu, Hao, and Ryne Estabrook. 2016. “Identification of Confirmatory Factor Analysis Models of Different Levels of Invariance for Ordered Categorical Outcomes.” <em>Psychometrika</em> 81 (4): 1014–45. doi:<a href="https://doi.org/10.1007/s11336-016-9506-0">10.1007/s11336-016-9506-0</a>.</p>
</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>There are analogous procedures using differential item functioning in an item response theory paradigm (IRT). For example, have a look at the <a href="https://cran.r-project.org/web/packages/difR/difR.pdf"><code>difR</code> package</a> on CRAN for more info.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>You can find release notes, tutorials, and much more on the <a href="http://lavaan.ugent.be">project’s page</a>.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
